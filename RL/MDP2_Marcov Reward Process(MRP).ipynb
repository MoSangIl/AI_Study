{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDP2 Markov Reward Process(MRP)\n",
    "\n",
    "MDP로 가기 위한 전 단계\n",
    "\n",
    "- Markov Chain with a Reward r\n",
    "- State to State 의 sequence 처럼 Reward 에대한 sequence 도 존재\n",
    "- Discount factor $\\gamma$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRP Definition\n",
    "**Definition: A MRP is a tuple $<S, P, R, \\gamma > $**\n",
    "\n",
    "- S, State의 유한 집합 (finite set of states)\n",
    "- P, State 간의 이동 확률 행렬 (state transition probability matrix)  \n",
    "$ P_{ss'} = P[S_{t+1} = s' | S_t = s]$  \n",
    "  \n",
    "- R, 보상 함수(Reward Function), $ R = E[R_{t+1} | S_t = s] $ state 's' 에서 시작했을 때 얻을 수 있는 Reward 기댓값\n",
    "- $\\gamma$, Discount factor $\\gamma \\in [0, 1]$\n",
    "\n",
    "ex)\n",
    "Student MRP\n",
    "![Student_MRP](./StudentMRP.png)\n",
    "\n",
    "빨간색으로 된 R 의 값이 각 State에서의 보상"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Return\" - Reward over Multiple Transitions\n",
    "\n",
    "*Definition: Return $G_t$ is the total discounted reward from time-step t*\n",
    "\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\cdots = \\sum_{k=0}^{\\infty}{\\gamma^kR_{t+k+1}}$$\n",
    "\n",
    "$R_{t+1}$ 해당 time에 얻게 되는 immediate reward!  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discount Factor $\\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reasonable(합리적이다.) Both 보상의 합의 최대화 And 미래의 보상 보다 현재의 보상이 더 선호됨\n",
    "- 해결 -> rewards decay exponentially\n",
    "- Mathematically convenient (Return 과 value 가 발산하는 것을 피함)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Function\n",
    "\n",
    "- Value Function 은 State s 로부터 긴시간 에피소드가 진행 되었을 때 얻어진 Return의 기댓값이다.\n",
    "\n",
    "**Definition: Expected Return starting from State s**\n",
    "\n",
    "$$v(s) = E[ G_t \\ | \\ S_t = s]  = E [R_{t+1} + \\gamma R_{t+2} + \\cdots \\ | \\ S_t = s] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Value Function of MRP (Smart and Efficient <-> Naive)\n",
    "\n",
    "- Value Function $v(S_t)$ -> two parts\n",
    "    - Immediate reward $R_{t+1}$ at state $S_t$ 해당 State에서 받는 즉각 보상\n",
    "    - Discounted value of successor state $\\gamma v(S_{t+1})$  그 다음 State에서 부터의 Value Function에 $\\gamma$ 만큼 감쇄 된 값\n",
    "    \n",
    "$\n",
    "v(s) = E[G_t \\ | \\ S_t = s] \\\\  \n",
    "\\ \\ \\ \\ \\ \\ = E[R_{t+1} + \\gamma R_{t+2} + \\gamma^2R_{t+3} + \\cdots \\ | \\ S_t = s] \\\\\n",
    "\\ \\ \\ \\ \\ \\ = E[R_{t+1} +\\gamma(R_{t+2} + \\gamma R_{t+3} + \\cdots) \\ | \\ S_t = s] \\\\\n",
    "\\ \\ \\ \\ \\ \\ = E[R_{t+1} + \\gamma G_{t+1} \\ | \\ S_t = s] \\\\\n",
    "\\ \\ \\ \\ \\ \\ = E[R_{t+1} + \\gamma v(S_{t+1}) \\ | \\ S_t = s] \\ (Bellman Equation)\n",
    "$\n",
    "로 유도 될 수 있다.  \n",
    "즉, 현재 State 로 부터의 Value 는 다음 State에 대한 Reward 그리고 다음 State 에서 Value의 합인 것.  \n",
    "  \n",
    "Bellman Equation for MRP 를 도출 해낸다.  \n",
    "$$v(s) = R_s + \\gamma \\sum_{s'\\in S}{P_{ss'}v(s')}$$\n",
    "\n",
    "보상 + 다음 스테이트에 대한 Value 기댓값 {Discount 모든스테이트에 대한 합계 (다음스테이트 에서의 Value * 넘어갈 확률)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Equation in Matrix Form\n",
    "\n",
    "$v = R + \\gamma Pv$ (행렬식)\n",
    "\n",
    "$$ \n",
    "\\begin{bmatrix} v(1)  \\\\ \\vdots \\\\ v(n) \\end{bmatrix} = \n",
    "\\begin{bmatrix} R_1  \\\\ \\vdots \\\\ R_n \\end{bmatrix} + \\gamma\n",
    "\\begin{bmatrix} p_{11} & \\cdots & p_{1n}  \\\\ \\vdots \\\\ p_{n1} & \\cdots & p_{nn} \\end{bmatrix}\n",
    "\\begin{bmatrix} v(1)  \\\\ \\vdots \\\\ v(n) \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "위 행렬식으로, Fixed Point Iteration 수치적 계산을 통해 계산할 수 있다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.41271\n"
     ]
    }
   ],
   "source": [
    "# [C1 C2 C3 Pass Pub FB Sleep] = [0 1 2 3 4 5 6]\n",
    "\n",
    "R = [-2, -2, -2, 10, 1, -1, 0]\n",
    "gamma = 0.9\n",
    "\n",
    "# if a sequence is given (Episode\n",
    "S = [0, 1, 2, 4, 2, 4]\n",
    "\n",
    "G = 0 # Return\n",
    "\n",
    "for i in range(6):\n",
    "    G = G + (gamma**i)*R[S[i]] # gamma를 적용한 return 계산\n",
    "    \n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 6, 6, 6, 6]\n",
      "-3.8\n"
     ]
    }
   ],
   "source": [
    "P = [[0, 0.5, 0, 0, 0, 0.5, 0],\n",
    "    [0, 0, 0.8, 0, 0, 0, 0.2],\n",
    "    [0, 0, 0, 0.6, 0.4, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 1],\n",
    "    [0.2, 0.4, 0.4, 0, 0, 0, 0],\n",
    "    [0.1, 0, 0, 0, 0, 0.9, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 1]]\n",
    "\n",
    "R = [-2, -2, -2, 10, 1, -1, 0]\n",
    "gamma = 0.9\n",
    "\n",
    "#  sequence generated by Markov chain\n",
    "# [C1 C2 C3 Pass Pub FB Sleep] = [0 1 2 3 4 5 6]\n",
    "\n",
    "# starting from 0\n",
    "x = 0\n",
    "S = []\n",
    "S.append(x)\n",
    "\n",
    "for i in range(5):\n",
    "    x = np.random.choice(len(P), 1, p=P[x][:])[0]\n",
    "    S.append(x)\n",
    "\n",
    "G = 0\n",
    "for i in range(5):\n",
    "    G = G + (gamma**i)*R[S[i]]\n",
    "print(S)\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student MRP With Values\n",
    "![student_MRP_with_values](./StudentMRP_with_value.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Function 대수적 방법\n",
    "\n",
    "$$ v = R + \\gamma Pv \\Longleftrightarrow\n",
    "(I - \\gamma P)v = R \\Longleftrightarrow\n",
    "v = (I - \\gamma P)^{-1} R\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.01272891]\n",
      " [ 0.9426553 ]\n",
      " [ 4.08702125]\n",
      " [10.        ]\n",
      " [ 1.90839235]\n",
      " [-7.63760843]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "P = [[0, 0.5, 0, 0, 0, 0.5, 0],\n",
    "    [0, 0, 0.8, 0, 0, 0, 0.2],\n",
    "    [0, 0, 0, 0.6, 0.4, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 1],\n",
    "    [0.2, 0.4, 0.4, 0, 0, 0, 0],\n",
    "    [0.1, 0, 0, 0, 0, 0.9, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 1]]\n",
    "\n",
    "R = [-2, -2, -2, 10, 1, -1, 0]\n",
    "\n",
    "P = np.asmatrix(P)\n",
    "R = np.asmatrix(R)\n",
    "R = R.T # Transpose\n",
    "\n",
    "gamma = 0.9\n",
    "\n",
    "v = (np.eye(7) - gamma*P).I*R\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Point Iteration 수치해석적 방법 (효율적 계산)\n",
    "\n",
    "$$v_{k+1}(s) \\longleftarrow R(s) + \\gamma \\sum_{s'\\in S}{p(s'|s)v_k(s')}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.01073295]\n",
      " [ 0.9428713 ]\n",
      " [ 4.08727922]\n",
      " [10.        ]\n",
      " [ 1.90900857]\n",
      " [-7.63401027]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "\n",
    "v = np.zeros([7, 1])\n",
    "for i in range(50):\n",
    "    v = R + gamma*P*v\n",
    "    \n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
