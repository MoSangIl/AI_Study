{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-function \n",
    "\n",
    "MDP -> State - Action Value function == Q-function\n",
    "\n",
    "- Q-function  \n",
    "$Q_{\\pi}(s, a) = R(s) + \\gamma \\sum_{s'\\in S}{P(s' \\ | \\ s, a)Q_{\\pi}(s', \\pi(s'))}$  \n",
    "$Q_{\\pi}(s, a) = R(s) + \\gamma \\sum_{s'\\in S}{P(s' \\ | \\ s, a)max_{a'}Q_*(s', a')}$  \n",
    "$ = R(s) + \\gamma \\sum_{s'\\in S}{P(s' \\ | \\ s, a)v_*(s')}$  \n",
    "\n",
    "Q-function 은 위와 같이 v 함수와의 관계를 가진다. MDP에서의 관계와 같은 맥락이다.\n",
    "\n",
    "- Optimal policy  \n",
    "$\\pi_*(s) = argmax_a \\sum_{s'}{P(s' \\ | \\ s,a)v_*(s')}$ 이므로,  \n",
    "\n",
    "\n",
    "$$\\pi_*(s) = argmax_aQ_*(s, a) without knowing dynamics(Model P)$$\n",
    "\n",
    "최종적으로, optimal policy 를 Q-function을 사용하여 구할 수 있다  \n",
    "\n",
    "\n",
    "**그럼 이제, Q-function을 어떻게 구할 수 있을까??**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-function 구하는 방법론\n",
    "\n",
    "- TD방법론과 유사하다. (Smoothing Effecter Alpha).   \n",
    "\n",
    "- Observe State s, Reward r, Action a (but not necessarily a = $\\pi(s)$)\n",
    "\n",
    "### SARSA: Estimate $Q_{\\pi}(s, a)$ for expectation\n",
    "\n",
    "$Q_{\\pi}(s, a) = R9s) + \\gamma \\sum_{s'\\in S}{P(s' \\ | \\ s, a)Q_{\\pi}(s', \\pi(s'))}$\n",
    "\n",
    "$$Q_{\\pi}(s, a) \\leftarrow (1 - \\alpha)(Q_{\\pi}(s, a)) + \\alpha(R_t + \\gamma Q_{\\pi}(s', \\pi(s'))$$\n",
    "\n",
    "SARSA = State - Action - Reward - State - Action. \n",
    "\n",
    "Greedy policy -> $\\pi'(s) = argmax_aQ_{\\pi}(s, a)$  \n",
    "\n",
    "### Q-learning: Estimate $Q_*(s, a)$ for optimality\n",
    " \n",
    "$Q_{\\pi}(s, a) = R9s) + \\gamma \\sum_{s'\\in S}{P(s' \\ | \\ s, a)max_{a'}Q_*(s', a')}$  \n",
    "\n",
    "$$Q_*(s, a) \\leftarrow (1 - \\alpha)(Q_*(s, a)) + \\alpha(R_t + \\gamma max_{a'}Q_*(s', a'))$$\n",
    "\n",
    "\n",
    "궁극적으로, Q-learning (optimal)을 구현해야 하는 것이다.  \n",
    "\n",
    "Optimal policy -> $\\pi_*(s) = argmax_aQ_*(s, a)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving Q-value (Estimate Q-function)\n",
    "\n",
    "- Q-value Iteration\n",
    "\n",
    "$$Q_{k+1}(s,a) \\leftarrow E_{s'~P(s'|s,a)}[R(s) + \\gamma max_{a'}Q_k(s', a')]$$\n",
    "\n",
    "Q_value 는 곧 Expectation 값으로 재정의 되어 진다.  \n",
    "\n",
    "그러나, 확률 전이 행렬을 알 수 없다는 것이 전제이므로! \"Replace Expectation by samples\" 여러 샘플중 하나를 선택하겠다! -> 곧 Policy를 정한 다는 의미와 같다.  \n",
    "\n",
    "**이에 따른 Q-Learning Algorithm**  \n",
    "1. state-action pair(s,a) 를 위한 state s, 혹은 a 를 샘플에서 가져온다  \n",
    "\n",
    "2. 가져온 state-action pair(s,a)로 $Q_k(s,a)$ 를 구한다.  \n",
    "\n",
    "3. 그 다음 State 대한 target(=Q-value(s'))를 구한다. \n",
    "$$target(s') = R(s) + \\gamma max_{a'}Q_k(s', a')$$\n",
    "\n",
    "4. TD 기법 을 사용하여, Q-function을 업데이트 한다.  \n",
    "$$Q_*(s, a) \\leftarrow (1 - \\alpha)(Q_*(s, a)) + \\alpha(R_t + \\gamma max_{a'}Q_*(s', a'))$$\n",
    "\n",
    "### How to Sample Actions (Exploration vs. Exploitation)\n",
    "\n",
    "**$\\epsilon$-Greedy**  \n",
    "\n",
    "Q-Value 를 구하고, 그에 대한 Greedy 한 값 즉 최댓값을 계속해서 선택하는 방법(Exploitation)은 안전하지만 더 나은 선택에 대한 가능성을 배제하는 것일 수도 있다.   \n",
    "\n",
    "따라서 $\\epsilon$만큼의 확률을 다른 경우의 수(최댓값x) 를 선택하는 데에 사용하고(Exploration)/ $1-\\epsilon$만큼의 확률은 Greedy하게 선택(Exploitation)한다.\n",
    "\n",
    "\n",
    "**$\\epsilon-Greedy$를 적용한 Q-Learning Algorithm**  \n",
    "\n",
    "1. Initialize Q(s, a) arbitrary (based on random choice or personal whim, rather than any reason or system.). \n",
    "\n",
    "2. 주어진 episode로부터 Choose Action a  \n",
    "\n",
    "3. Take action a, observe R, s'. \n",
    "\n",
    "4. Q-Value 갱신한다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MDP Process\n",
    "![MDP_PROCESS](resource/MDPprocess.png)\n",
    "\n",
    "Environment! -> 기존 MDP로 모델링 그러나 이젠 MDP 모름\n",
    "\n",
    "Agent 가 Action을 취하면 해당 Action에 따라 다음 State s, Reward r만 Environment 는 제시 (Black box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question \n",
    "- Sample 은 우리가 정하는 것인가?  Yes..\n",
    "Sample 이란 Episode를 말한다. 그건 우리가 주는 것!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
