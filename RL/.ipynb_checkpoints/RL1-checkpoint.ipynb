{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "MDP 에서 강화학습의 내용이 어떻게 연결 되는가?!\n",
    "\n",
    "- The RL twist : wd do not know P or R.. 모든 Action에 대해 Probability Matrix는 사전에 잘 알 수가 없다.  \n",
    "여기서, 강화학습은 P를 알지 못하더라도 MDP 와 같은 알고리즘으로 진행 되어 학습한 다는 것을 알자.\n",
    "\n",
    "- They are too big to enumerate (only have the ability to act in MDP, observe states and rewards)  경우의 수가 너무 많아진다.  \n",
    "-> deep RL으로 개념이 확장되는 계기  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving MDP\n",
    "\n",
    "- (Policy evaluation) Determine value of policy $\\pi$  \n",
    "$$ v_{\\pi}(s) = R(s) + \\gamma \\sum_{s'\\in S}{P(s' \\ | \\ , \\pi(s))v_{\\pi}(s')}$$\n",
    "\n",
    "- (Value Iteration) Determine value of optimal policy\n",
    "$$v_*(s) = R(s) + \\gamma \\sum_{s'\\in S}{P(s' \\ | \\ s, a)v_*(s')} $$\n",
    "\n",
    "### Optimal policy\n",
    "$$\\pi_*(s) = argmax_{a\\in A} \\sum_{s'\\in S}{P(s' \\ | \\ s, a)v_*(s')}$$\n",
    "\n",
    "How.. can we compute these quantities when P and R are unknow?\n",
    "- RL\n",
    "    1. model-based RL => MDP\n",
    "    2. model-free RL \n",
    "        - Value-based\n",
    "        - Policy-based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-based RL\n",
    "\n",
    "**Just Estimate The MDP From Data (known as Monte Carlo[MC] method)**\n",
    "\n",
    "- Agent acts in the work! ovserves episodes of experience = 어떤 polic에 따라 움직이게 된다. 그리고 그 경험의 에피소드를 Observe 하게 된다.\n",
    "- We form the empirical estimate of the MDP via the counts = 실질 적인 것이 아니라 경험적으로 알 수 있게 된다. \n",
    "\n",
    "*Summary*\n",
    "\n",
    "\n",
    "- Will converge to correct MDP (hence correct value function/policy) given enough samples of each state / 충분히 많은 에피소드가 있게 되면, 알맞은 , value / policy를 얻을 수 있다.\n",
    "- How can we ensure we get the 'right' samples? -> Challenging Problem!! True 한 데이터 샘플들을 확실하게 갖기는 어려운 일이다! Probability Matrix를 잘 정의할 수 없기 때문.\n",
    "- Advantages(informally): makes\"efficient\" use of data\n",
    "- Disadvantages: 실제 MDP 모델을 만들필요가 있다. (P, R 등 데이터 필요) -> State space 가 더 커지면 더많은 데이터 샘플이 필요하게 되므로, 효과적이지 않다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-free RL\n",
    "\n",
    "**Temporal difference methods(TD, SARSA, Q-learning): directly learn value function $v_{\\pi} or v_*$**  \n",
    "\n",
    "$$ v_{\\pi}(s) = R(s) + \\gamma \\sum_{s'\\in S}{P(s' \\ | \\ , \\pi(s))v_{\\pi}(s')}$$\n",
    "\n",
    "위 식의 계산은 한 State 에서 가능한 모든 State들로의 Transition 경우를 포함하여 평균적으로 계산한 식이 된다.  \n",
    "\n",
    "그러나 Model-free RL 은 Transition Probability 데이터 없이 주어진 정책에 따라 Value를 구해야 한다. 이는 결국 $s_t -> s_{t+1}$ 이 단일한 transtion이 일으킨 그 다음 State에서의 value를 구할 수 있다는 것이다.  \n",
    "\n",
    "따라서 평균적인 Value를 구할 수 없다. 즉 optimal value 를 구하지 못한다.  \n",
    "\n",
    "$\\alpha < 1 개념을 도입하여 Smooth 한 value 값을 저장한다.$\n",
    "\n",
    "$$v_{\\pi}(s_t) \\leftarrow (1-\\alpha)(v_{\\pi}(s_t)) + \\alpha(R_t + \\gamma v_{\\pi}(s_{t+1}))$$\n",
    "\n",
    "즉, 이전 Value 값과 다음 Transition에 의해 구해진 Value에 대해 비중을 다르게 하여 값을 정한다.  \n",
    "\n",
    "**Direct policy search: directly learn optimal policy $\\pi_*$**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entering Q Function (= State - Action Value Function)\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
