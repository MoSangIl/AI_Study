{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "MDP 에서 강화학습의 내용이 어떻게 연결 되는가?!\n",
    "\n",
    "- The RL twist : wd do not know P or R.. 모든 Action에 대해 Probability Matrix는 사전에 잘 알 수가 없다.  \n",
    "여기서, 강화학습은 P를 알지 못하더라도 MDP 와 같은 알고리즘으로 진행 되어 학습한 다는 것을 알자.\n",
    "\n",
    "- They are too big to enumerate (only have the ability to act in MDP, observe states and rewards)  경우의 수가 너무 많아진다.  \n",
    "-> deep RL으로 개념이 확장되는 계기  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving MDP\n",
    "\n",
    "- (Policy evaluation) Determine value of policy $\\pi$  \n",
    "$$ v_{\\pi}(s) = R(s) + \\gamma \\sum_{s'\\in S}{P(s' \\ | \\ , \\pi(s))v_{\\pi}(s')}$$\n",
    "\n",
    "- (Value Iteration) Determine value of optimal policy\n",
    "$$v_*(s) = R(s) + \\gamma \\sum_{s'\\in S}{P(s' \\ | \\ s, a)v_*(s')} $$\n",
    "\n",
    "### Optimal policy\n",
    "$$\\pi_*(s) = argmax_{a\\in A} \\sum_{s'\\in S}{P(s' \\ | \\ s, a)v_*(s')}$$\n",
    "\n",
    "How.. can we compute these quantities when P and R are unknow?\n",
    "- RL\n",
    "    1. model-based RL => MDP\n",
    "    2. model-free RL \n",
    "        - Value-based\n",
    "        - Policy-based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-based RL\n",
    "\n",
    "**Just Estimate The MDP From Data (known as Monte Carlo[MC] method)**\n",
    "\n",
    "- Agent acts in the work! ovserves episodes of experience = 어떤 polic에 따라 움직이게 된다. 그리고 그 경험의 에피소드를 Observe 하게 된다.\n",
    "- We form the empirical estimate of the MDP via the counts = 실질 적인 것이 아니라 경험적으로 알 수 있게 된다. \n",
    "\n",
    "*Summary*\n",
    "\n",
    "\n",
    "- Will converge to correct MDP (hence correct value function/policy) given enough samples of each state / 충분히 많은 에피소드가 있게 되면, 알맞은 , value / policy를 얻을 수 있다.\n",
    "- How can we ensure we get the 'right' samples? -> Challenging Problem!! True 한 데이터 샘플들을 확실하게 갖기는 어려운 일이다! Probability Matrix를 잘 정의할 수 없기 때문.\n",
    "- Advantages(informally): makes\"efficient\" use of data\n",
    "- Disadvantages: 실제 MDP 모델을 만들필요가 있다. (P, R 등 데이터 필요) -> State space 가 더 커지면 더많은 데이터 샘플이 필요하게 되므로, 효과적이지 않다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
