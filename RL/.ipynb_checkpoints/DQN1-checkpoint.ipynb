{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN (Deep Q-Learning Network)\n",
    "\n",
    "## Preview: Problem of Q-Learning\n",
    "\n",
    "- Not scalable! Must compute tabular $Q(s, a)$ for every state-action pair 너무 많은 State의 개수 경우 practical 하지 않다. \n",
    "\n",
    "- Solution: use a \"function approximation\" to estimate $Q(s,a)$\n",
    "    - Deep Learning의 방법론을 가지고 접근하겠다. (Universal Function Approximate) \n",
    "    - Learn about some small number of training states from ex\n",
    "    - Generalize that experience to new, similar situations(일반화)\n",
    "    \n",
    "## Approximate Q-Learning\n",
    "\n",
    "$$target(s') = R(s) + \\gamma max_{a'}Q_w(s', a') \\\\\n",
    "Q_{k+1}(s, a) \\leftarrow Q_k(s,a) + \\alpha(target(s') - Q_k(s,a))$$\n",
    "\n",
    "- Linear function approximator\n",
    "$$Q_w(s,a) = w_0f_0(s,a) + w_1f_1(s,a) + \\dots + w_nf_n(s,a)$$\n",
    "\n",
    "-> 결국 w의 값을 업데이트 시켜주는 방법이 된다. (Train Parameters)  \n",
    "\n",
    "\n",
    "- Exact Q\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha [difference]$$\n",
    "\n",
    "\n",
    "- Approximate Q\n",
    "$$w_i \\leftarrow w_i + \\alpha[difference] f_i(s, a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Function Approximator\n",
    "\n",
    "- Approximate Q\n",
    "$$w_i \\leftarrow w_i + \\alpha[difference] f_i(s, a)$$\n",
    "\n",
    "위 식을 Loss function 으로부터 Gradient Descent 의 최소화 방법을 사용하여 유도해보자   \n",
    "\n",
    "\n",
    "- Loss function\n",
    "$$ l = \\cfrac{1}{2}(y - \\hat{y})^2$$\n",
    "\n",
    "실측값 - 예측값 의 제곱 - 미분을 위한 1/2 장치. \n",
    "\n",
    "- Difference\n",
    "$$y - \\hat{y} = y - (w_0f_0(x) + w_1f_1(x) + \\dots + w_nf_n(x)) = y(target) - \\sum_{i}{w_if_i(x)}(prediction)$$\n",
    "\n",
    "Linear Function Approximator 를 사용한 표현  \n",
    "\n",
    "- For one Point (Gradient Descent) 미분을 사용한 최소화\n",
    "\n",
    "$$\n",
    "\\cfrac{\\delta l(w)}{\\delta w_k} = -(y - \\sum_{i}{w_if_i(x)})f_k(x) \\\\\n",
    "w_k \\leftarrow w_i + \\alpha(y - \\sum_{i}{w_if_i(x)})f_k(x) \\\\\n",
    "\\therefore w_k \\leftarrow w_i + \\alpha [difference]f_k(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non_Linear Function Approximator : Deep Q-Learning Network\n",
    "\n",
    "- Difference  \n",
    "\n",
    "$$difference = [R(s) + \\gamma max_{a'}Q_w(s', a')] - Q_w(s, a)$$\n",
    "\n",
    "- Loss function by mean-squared error in Q-value\n",
    "\n",
    "$$l(w) = [\\cfrac{1}{2}(target(s') - Q_w(s, a))^2 ]$$\n",
    "\n",
    "- GD update:\n",
    "$$w_{k+1} \\leftarrow w_k -\\alpha \\triangledown_wl(w)  \\ \\ \\ \\Rightarrow Deep \\ Learning \\ Framework!! \\\\\n",
    "w_{k+1} \\leftarrow w_k - \\alpha \\triangledown_w[\\cfrac{1}{2}(target(s') - Q_w(s, a))^2 ] \\\\\n",
    "w_{k+1} \\leftarrow w_k + \\alpha[(target(s') - Q_w(s, a))] \\triangledown_wQ_w(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "- Deep Learning!! 그의 정의는 대체?\n",
    "\n",
    "- Q-Learning / DQN 에 대한 lab 을 잘 살펴서 정확히 이해가 필요하다!\n",
    "    - 쓰임새를 정확하게 하자!\n",
    "    \n",
    "- DP 알고리즘 공부 하자\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
