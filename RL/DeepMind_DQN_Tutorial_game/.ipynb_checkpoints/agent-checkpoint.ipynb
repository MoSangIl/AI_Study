{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요한 모듈 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game import Game\n",
    "from model import DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train mode & Replay(실행) mode 구분\n",
    "\n",
    "- 에이전트 -> 학습 모드 & 실행 모드\n",
    "- **학습모드** : 게임을 화면에 보여주지 않은 채, 학습속도를 높임\n",
    "- **게임 실행 모드** : 학습한 결과를 이용해, 게임을 진행하면서 화면에 출력\n",
    "\n",
    "- 이를 위해 tf.app.flags 를 이용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.app.flags.DEFINE_boolean(\"train\", False, \"학습모드. 게임을 화면에 보여주지 않습니다.\")\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPISODE = 10000\n",
    "TARGET_UPDATE_INTERVAL = 1000\n",
    "TRAIN_INTERVAL = 4\n",
    "OBSERVE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAX_EPISODE\n",
    "- 최대로 학습할 게임 횟수 (10000)\n",
    "\n",
    "### TRAIN_INTERVAL\n",
    "- 게임 4프레임(상태)마다 한 번씩 학습\n",
    "\n",
    "### OBSERVE\n",
    "- 일정 수준의 학습데이터가 쌓이기 전까지 학습 X : 학습데이터가 적을때는 학습의 효과가 크지 않다.\n",
    "- 100번의 프레임 지난 뒤부터 학습 진행\n",
    "\n",
    "### TARGET_UPDATE_INTERVAL\n",
    "\n",
    "- 학습을 '일정' 횟수만큼 진행할 때마다 '한번씩' 목표 신경망을 갱신하라\n",
    "- DQN의 안정된 학습을 위함 -> 학습 진행 중 최적의 행동을 얻어내는 \"기본신경망\", 그리고 이 값이 좋은 선택인지 비교하는 \"목표신경망\"의 분리\n",
    "- 가장 최근이나 오래된 학습 결과로 현재의 선택 비교시, 적절한 비교 X\n",
    "- 적당한 시점에 학습결과로 \"목표신경망\" 갱신"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 게임에 필요한 설정 (= 떨어지는 물건을 좌우로 움직이면서 피하는 게임)\n",
    "- 취할 수 있는 행동(NUM_ACTION) : 좌, 우, 상태유지 (3가지)\n",
    "- 게임 화면: 가로(SCREEN_WIDTH)6칸, 세로(SCREEN_HEIGHT) 10칸으로 설정\n",
    "- Game 모듈에 들어있어야 한다, 이해를 위해 에이전트에 넣음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ACTION = 3 # 행동 - 0: 좌, 1: 유지, 2: 우\n",
    "SCREEN_WIDTH = 6\n",
    "SCREEN_HEIGHT = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 에이전트 코드 -1) 학습\n",
    "- 텐서플로 세션과 게임 객체, 그리고 DQN모델 객체를 생성 (Session, Game, DQN)\n",
    "- 게임 객체(GAME): 화면 크기를 넣어줌\n",
    "- *show_game=FALSE* : 화면 출력X > 학습 속도 up\n",
    "- DQN 객체: 신경망 학습시키기 위해 텐서플로 세션을 넣어줌\n",
    "- 화면 입력! -> CNN 구성.. 화면크기 초기 설정!\n",
    "- **가장 중요! 신경망의 최종 결과값의 개수!: *선택할 행동의 개수* (NUM_ACTION) 넣어줌** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 에이전트 코드 -2) 학습 결과 저장 및 학습 상태 확인\n",
    "### (1) 에피소드별 점수 저장\n",
    "- *rewards* : 에피소드 마다 얻는 점수 저장, 확인하기 위한 텐서\n",
    "- 에피소드 10번에 한번씩 로그를 저장, *rewards* 평균 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 학습 결과 저장\n",
    "- 학습결과 저장 위한 *tf.train.Saver*와 텐서플로 세션, 로그 저장 위한 *tf.summary.FileWriter* 객체 생성\n",
    "- 학습 상태를 확인하기 위한 값들을 모아서 저장하기 위한 텐서를 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 에이전트 코드 -3) 목표 신경망 초기화\n",
    "- 목표 신경망 한 번 초기화\n",
    "- 아직 학습된 결과 X -> 초기 목표신경망은 기본신경망과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 에이전트 코드 -4) DQN 이용 시점 설정 및 진행 상태와 결과 저장 값 초기화\n",
    "### (1) DQN 이용 시점\n",
    "- 행동 선택 시 DQN을 이용할 시점!\n",
    "- 학습 초반에는 DQN이 항상 같은 값만 내놓을 가능성이 높음\n",
    "- 따라서 일정 시간이 지나기 전에는 행동을 무작위 선택!\n",
    "- epsilon 값을 \"줄여나가는\" 코드 입력\n",
    "\n",
    "### (2) 학습 진행 상태\n",
    "- 학습 진행을 조절하기 위해 진행된 프레임(상태) 횟수 초기화\n",
    "\n",
    "### (3) 점수 저장 배열\n",
    "- 학습 결과를 확인하기 위해 점수들을 저장할 배열을 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    # 학습\n",
    "    print(\"뇌세포 깨우는 중...\")\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    game = Game(SCREEN_WIDTH, SCREEN_HEIGHT, show_game=False)\n",
    "    brain = DQN(sess, SCREEN_WIDTH, SCREEN_HEIGHT, NUM_ACTION)\n",
    "    \n",
    "    # 학습 결과 저장 및 학습 상태 확인\n",
    "    tf.disable_eager_execution()\n",
    "    ## (1) 에피소드별 점수 저장\n",
    "    rewards = tf.placeholder(tf.float32, [None])\n",
    "    tf.summary.scalar('avg.reward/ep.', rf.reduce_mean(rewards))\n",
    "    \n",
    "    ## (2) 학습 결과 저장\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    writer = tf.summary.FileWriter('logs', sess.graph)\n",
    "    summary_merged = tf.summary.merge_all()\n",
    "    \n",
    "    # 목표신경망 초기화\n",
    "    brain.update_targert_networ()\n",
    "    \n",
    "    # DQN 이용 시점 설정 및 진행 상태와 결과 저장 값 초기화\n",
    "    \n",
    "    ## (1) DQN 이용 시점\n",
    "    epsilon = 1.0\n",
    "    \n",
    "    ## (2) 학습 진행 상태\n",
    "    time_step = 0\n",
    "    \n",
    "    ## (3) 점수 저장 배열\n",
    "    total_reward_list = []\n",
    "    \n",
    "    # 학습 진행 및 학습\n",
    "    for episode in range(MAX_EPISODE):\n",
    "        \n",
    "        ## (1) 게임 종료 상태\n",
    "        terminal = False\n",
    "        \n",
    "        ## (2) 한 게임당 얻은 총 점수\n",
    "        total_reward = 0\n",
    "        \n",
    "        ## (3) 게임 상태 초기화\n",
    "        state = game.reset()\n",
    "        \n",
    "        ## (4) DQN 초기 상태 설정\n",
    "        brain.init_state(state)\n",
    "        \n",
    "        # 최초 게임 진행\n",
    "        while not terminal:\n",
    "            \n",
    "            if np.random.rand() < epsilon: ## 입실론이 랜덤 값보다 큰 경우 랜덤한 액션!\n",
    "                action = random.randrange(NUM_ACTION)\n",
    "            else: ## Otherwise, DQN을 활용하여 액션 선택\n",
    "                action = brain.get_action()\n",
    "            \n",
    "            ## 일정 시간이 지난 뒤 입실론 값을 줄여감\n",
    "            if episode > OBSERVE:\n",
    "                epsilon -= 1 / 1000\n",
    "            \n",
    "            # 결정한 행동 이용 게임 진행\n",
    "            state, reward, terminal = game.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # 현재 상태 신경망 객체에 기억\n",
    "            brain.remember(state, action, reward, terminal)\n",
    "            \n",
    "            # 현재 프레임값별 학습 진행\n",
    "            if time_step > OBSERVE and time_step % TRAIN_INTERVAL == 0:\n",
    "                brain.train()\n",
    "            \n",
    "            if time_step % TARGET_UPDATE_INTERVAL == 0:\n",
    "                brain.update_target_network()\n",
    "            \n",
    "            time_step += 1\n",
    "            \n",
    "        # 에피소드 종료\n",
    "        print('게임횟수: %d 점수: %d' % (episode + 1, total_reward))\n",
    "        \n",
    "        total_reward_list.append(total_reward)\n",
    "        \n",
    "        if episode % 10 == 0:\n",
    "            summary = sess.run(summary_merged, feed_dict={rewards: total_reward_list})\n",
    "            writer.add_summary(summary, time_step)\n",
    "            total_reward_list = []\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            saver.save(sess, 'model/dqn.ckpt', global_step=time_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 에이전트 코드 -5) 학습 진행 및 학습\n",
    "- 게임을 진행하고 학습시키는 부분을 작성\n",
    "- 앞서 설정한 *MAX_EPISODE* 횟수만큼 게임 에피소드를 진행\n",
    "- 매 게임을 시작하기 전에 초기화\n",
    "\n",
    "### (1) 게임 종료 상태: *terminal*\n",
    "### (2) 한 게임당 얻은 총 점수: *total_reward*\n",
    "### (3) 게임 상태 초기화: *game.reset()*\n",
    "### (4) DQN 초기 상태값 설정\n",
    "- 초기화한 게임 상태를 DQN에 초기 상태값으로 넣어 줌\n",
    "- 상태는 *SCREEN_WIDTH* X *SCREEN_HEIGHT* 크기 화면 구성\n",
    "\n",
    "### NOTE\n",
    "- 원래 DQN에서는 픽셀값들을 상태값으로 받음\n",
    "- 여기서 Game 모듈에서 학습 속도를 높이고자 해당 위치에 사각형이 있는지 없는지를 1 / 0 으로 전달"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 에이전트 코드 -6) 최초 게임 진행\n",
    "- 게임 에피소드 한 번 진행\n",
    "- 녹색 사각형 충돌 시까지 진행\n",
    "- 게임에서 처음으로 해야 할 일: 다음에 취할 행동을 선택하는 일\n",
    "- 학습 초반, 행동 무작위 선택\n",
    "- 일정시간(OBSERVE) 지난 뒤 epsilon 값을 조금씩 줄여나감 : 무작위가 점점 사용되지 않음\n",
    "- 하이퍼파라미터.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 에이전트 코드 -7) 결정한 행동 이용 게임 진행\n",
    "- 앞서 결정한 행동을 이용해 게임을 진행하고, 보상과 게임의 종료 여부를 받아옴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 에이전트 코드 -8) 현재 상태 신경망 객체에 기억\n",
    "- 현재 상태를 신경망 객체에 기억\n",
    "- 이 기억한 현재 상태를 이용해 다음 상태에서 취할 행동을 정함\n",
    "- 여기서 저장한 상태, 행동, 보상 을 이용하여 신경망을 학습!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 에이전트 코드 -9) 현재 프레임값별 학습 진행\n",
    "- 현재 프레임 (OBSERVE: 100)이 넘으면 (TIME_INTERVAL: 4) 마다 한번씩 학습!\n",
    "- (TARGET_UPDATE_INTERVAL) 마다 한번씩 목표신경망 갱신"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 에이전트 코드 -10) 에피소드 종료\n",
    "- 사각형들이 충돌해 에피소드가 종료되면, 획득한 점수 출력! 이번 에피소드에서 받은 점수를 저장\n",
    "- 에피소드 10번 마다 받은 점수를 로그에 저장\n",
    "- 100번 마다 학습된 모델을 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 결과 실행 함수 -1) 결과 실행\n",
    "- 결과를 실행하는 *replay()* 함수는 학습부분만 빠져있을 뿐 *train()* 함수와 거의 같음\n",
    "- 주의 할점! -> 텐서플로 세션 새로 생성 X, tf.train.Saver() 로 저장해둔 모델을 읽어와서 생성!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 결과 실행 함수 -2) 게임 진행\n",
    "- 게임을 진행하는 부분\n",
    "- 학습 코드 빠짐\n",
    "- 인간이 인지 가능 속도! 마지막 부분 time.sleep(0.3) 코드 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay():\n",
    "    \n",
    "    # 결과 실행\n",
    "    print('뇌세포 깨우는 중...')\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    game = Game(SCREEN_WIDTH, SCREEN_HEIGHT, show_game=True)\n",
    "    brain = DQN(sess, SCREEN_WIDTH, SCREEN_HEIGHT, NUM_ACTION)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    ckpt = tf.train.get_checkpoint_state('model')\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    \n",
    "    # 게임 진행\n",
    "    for episode in range(MAX_EPISODE):\n",
    "        terminal = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        state = game.reset()\n",
    "        brain.init_state(state)\n",
    "        \n",
    "        while not terminal:\n",
    "            action = brain.get_action()\n",
    "            state, reward, terminal = game.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            brain.remember(state, action, reward, terminal)\n",
    "            \n",
    "            time.sleep(0.3)\n",
    "        \n",
    "        print('게임횟수: %d 점수: %d' % (episode + 1, total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 게임 진행 방법 선택\n",
    "- 스크립트 학습용 실행, 학습된 결과 게임 실행..\n",
    "- 터미널에서 스크립트 실행 시 train 옵션을 받아 정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "뇌세포 깨우는 중...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d5939b4c1e66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mmain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags_parser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_parse_flags_tolerate_undef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/absl/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv, flags_parser)\u001b[0m\n\u001b[1;32m    297\u001b[0m       \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m       \u001b[0m_run_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mUsageError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m       \u001b[0musage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshorthelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetailed_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexitcode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/absl/app.py\u001b[0m in \u001b[0;36m_run_main\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-d5939b4c1e66>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-36c42c2a90fd>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSCREEN_WIDTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSCREEN_HEIGHT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_game\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mbrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSCREEN_WIDTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSCREEN_HEIGHT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_ACTION\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI_Studying/RL/DeepMind_DQN_Tutorial_game/game.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, screen_width, screen_height, show_game)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshow_game\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prepare_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AI_Studying/RL/DeepMind_DQN_Tutorial_game/game.py\u001b[0m in \u001b[0;36m_prepare_display\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_size_inches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# 화면을 닫으면 프로그램을 종료합니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmpl_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'close_event'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         plt.tick_params(top='off', right='off',\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main(_):\n",
    "    if FLAGS.train:\n",
    "        train()\n",
    "    else:\n",
    "        replay()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
