{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDP Markov Decision Process\n",
    "\n",
    "**Definition: A MDP is a tuple <S, A, P, R, r>**\n",
    "\n",
    "- (Plus from MRP) A is a finite set of actions\n",
    "    - $P_{ss'}^a = P[S_{t+1} = s' | S_t = s, A_t = a]$\n",
    "    - $R_s^a = E[R_{t+1} | S_t = s, A_t = a]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Environment -> States are Markov*\n",
    "\n",
    "Environmnet - Markov Chain  \n",
    "Agent - Action을 취할 수 있는 권한이 있다.\n",
    "\n",
    "Now, it is Active not Passive\n",
    "\n",
    "Important Process of MDP\n",
    "![MDP_process](./MDPprocess.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deterministic VS Stochastic\n",
    "\n",
    "Deterministic : Cirtain 갈 곳이 하나로 정해져 있다. (p = 1)  \n",
    "\n",
    "Stochastic : Uncirtain  여러 State로 갈 수 있다. (p1 = 0.2, p2 = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy  $\\pi$\n",
    "\n",
    "policy 란 어떤 State에서 어떤 Action을 취할 것인지 Mapping 하는 것  \n",
    "\n",
    "-> 이때, 각 State 에서 Action이 고정되기 때문에 MDP -> MRP 가 된다.  \n",
    "\n",
    "**Policy 는 Deterministic 할 수도, Stochastic 할 수 도 있다.**  \n",
    "이때, Policy 행렬 -> $\\pi(a|s)$\n",
    "\n",
    "- Policy 는 현재 상태에 의존적이다.\n",
    "- Policy 는 시간에 독립적이고, Policies are stationary ->(Optimal)\n",
    "\n",
    "$$P^\\pi \\ is \\ a \\ matrix \\ containing \\ probabilities \\ for \\ each \\ transition \\ under \\ policy \\ \\pi$$\n",
    "\n",
    "즉, A + P -> $P^\\pi$\n",
    "\n",
    "### Question?\n",
    "- How many possible policies in our example?\n",
    "Action 에 따라 가능 한 policy는 여러가지가 될 수 있다!\n",
    "\n",
    "- Which of the above two policies is best?\n",
    "\n",
    "- How do you compute the optimal policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Function\n",
    "\n",
    "### State-Value Function $v_{\\pi}(s)$\n",
    "State s 로부터 policy $\\pi$를 취하여 진행한 value function  \n",
    "\n",
    "$v_{\\pi}(s) = E_{\\pi}[G_t \\ | \\ S_t = s]  \\\\\n",
    "\\ \\ \\ \\ \\ \\  \\ \\ = E_{\\pi}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) \\ | \\ S_t = s]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action-Value Function $q_{\\pi}(s)$\n",
    "\n",
    "현재 스테이트에서는 Action a 를 선택하고 다음 s' 부터는 policy $\\pi$ 따르도록 한다.  \n",
    "\n",
    "\n",
    "$q_{\\pi}(s) = E_{\\pi}[G_t \\ | \\ S_t = s, A_t = a]  \\\\\n",
    "\\ \\ \\ \\ \\ \\  \\ \\ = E_{\\pi}[R_{t+1} + \\gamma q_{\\pi}(S_{t+1}, A_{t+1}) \\ | \\ S_t = s, A_t = a]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Flow of transition between states following the Policy\n",
    "![Policy_process](resource/PolicyProcess.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Relationship between $v_{\\pi}(s) \\ And \\ q_{\\pi}(s)$**\n",
    "$$v_{\\pi}(s) = \\sum_{a\\in A}{\\pi (a|s)q_{\\pi}(s,a)}$$\n",
    "![State_to_Action](resource/StateToActionVF.png)\n",
    "State Value Function 은 State s 부터 취할 수 있는 모든 Action을 따지지만, Action Value Function 은 State s 에서는 Action a를 취하고 그 이후부터 policy 를 따르게 한다.  \n",
    "\n",
    "$$ q_{\\pi}(s,a) = R(s) + \\gamma \\sum_{s'\\in S}{P(s'|s, a)v_{\\pi}(s')} $$\n",
    "![State_to_Action](resource/ActionToStateVF.png)\n",
    "R(s) = 해당 State에 대한 보상! immediate Reward  \n",
    "Value function -> 해당 State s로부터 끝까지 얻어진 Return(G) 에대한 기댓값  \n",
    "\n",
    "이때, P 가 State Transition Matrix 이다. \n",
    "\n",
    "Conclude, *State(Reward) -> Policy -> Transition -> State'*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Optimality Equation\n",
    "\n",
    "### SVF $v_{\\pi}(s)$\n",
    "\n",
    "**The Optimal state-value function $v_*(s)$ = maximum value function over policies**  \n",
    "\n",
    "### AVF $q_{\\pi}(s)$\n",
    "\n",
    "**The Optimal state-value function $q_*(s)$ = maximum value function over policies**  \n",
    "\n",
    "### Optimal Policy\n",
    "$\\pi_*(s) = argmax_{\\pi}v_{\\pi}(s)$\n",
    "argmax 란 State Value Function 이 최적일때의 pi 값을 반환한다는 것이다.   \n",
    "따라서, $v_*(s), q_*(s,a)$를 알면 최적의 policy pi 도 구할 수 있다.   \n",
    "\n",
    "$$\\pi_*(a | s) = 1 \\ if \\ a = argmax_{a\\in A} q_*(s, a) \\ / \\ 0 \\ otherwise$$  \n",
    "\n",
    "- There is always a deterministic optimal policy for an MDP  \n",
    "\n",
    "- If we know $q_*(s,a)$, we can have the optimal policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "- Algorithm\n",
    "\n",
    "1. Initialize an estimate for the value function arbitarily (or zeros)\n",
    "$$ v(s) \\leftarrow 0 \\ s \\in S $$\n",
    "2. Repeat, update\n",
    "$$ v(s) \\leftarrow R(s) + \\gamma max_a \\sum_{s'\\in S}{P(s' \\ | \\ s,a)v(s')}, s \\in S $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "- Given a policy $\\pi$ then evaluate the policy $\\pi$\n",
    "- Improve the policy by acting greedily with respect to $v_{\\pi}$\n",
    "\n",
    "**Update $\\pi$ to be *greedy policy* with respect to $v_{\\pi}$**\n",
    "\n",
    "$$\\pi(s) \\leftarrow argmax_a \\sum_{s'\\in S}{P(s' \\ | \\ s,a)v_{\\pi}(s')}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principle of Optimality\n",
    "\n",
    "- Shortest Path  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab\n",
    "\n",
    "1st Example Image\n",
    "![Example1](resource/1stExample_Lab.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.        ]\n",
      " [ 0.        ]\n",
      " [18.18181818]\n",
      " [10.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Naive\n",
    "import numpy as np\n",
    "\n",
    "P = [[1, 0, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0.5, 0, 0.5, 0],\n",
    "    [0, 1, 0, 0]]\n",
    "\n",
    "R = [0, 0, 10, 10]\n",
    "\n",
    "P = np.asmatrix(P)\n",
    "R = np.asmatrix(R)\n",
    "R = R.T\n",
    "gamma = 0.9\n",
    "v = (np.eye(4) - gamma*P).I*R\n",
    "\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ v(s) \\leftarrow R(s) + \\gamma \\sum_{s'\\in S}{P(s' \\ | \\ s,a)v(s')} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.        ]\n",
      " [ 0.        ]\n",
      " [18.17562716]\n",
      " [10.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Iterative\n",
    "v = np.zeros([4,1])\n",
    "\n",
    "for _ in range(10):\n",
    "    v = R + gamma*P*v\n",
    "    \n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Optimality!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [0, 1, 2, 3]\n",
    "actions = [0, 1]\n",
    "P = {\n",
    "    0: {\n",
    "        0: [(1, 0)],\n",
    "        1: [(0.5, 0), (0.5, 1)]\n",
    "    },\n",
    "    1: {\n",
    "        0: [(0.5, 0), (0.5, 3)],\n",
    "        1: [(1, 1)]\n",
    "    },\n",
    "    2: {\n",
    "        0: [(0.5, 0), (0.5, 2)],\n",
    "        1: [(0.5, 0), (0.5, 1)]\n",
    "    },\n",
    "    3: {\n",
    "        0: [(0.5, 2), (0.5, 3)],\n",
    "        1: [(1, 1)]\n",
    "    }\n",
    "}\n",
    "\n",
    "R = [0, 0, 10, 10]\n",
    "\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.5, 0), (0.5, 2)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[2][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_{s'\\in S}{P(s' \\ | \\ s,a)v(s')} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "# compute the above summation\n",
    "\n",
    "#s = 2, a = 0\n",
    "\n",
    "v = [0, 0, 10, 10]\n",
    "\n",
    "tmp = 0\n",
    "\n",
    "for trans in P[2][0]:\n",
    "    tmp += trans[0]*v[trans[1]]\n",
    "    \n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shorten\n",
    "\n",
    "sum(trans[0]*v[trans[1]] for trans in P[2][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "$$ v(s) \\leftarrow R(s) + \\gamma max_a \\sum_{s'\\in S}{P(s' \\ | \\ s,a)v(s')}, s \\in S $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31.58508953413495, 38.60400287377479, 44.02416232966445, 54.20158563176306]\n"
     ]
    }
   ],
   "source": [
    "# optimal value function\n",
    "\n",
    "# v = [0]*4\n",
    "v = [0, 0, 0, 0]\n",
    "\n",
    "for _ in range(100):\n",
    "    for s in states:\n",
    "        q_0 = sum(trans[0]*v[trans[1]] for trans in P[s][0])\n",
    "        q_1 = sum(trans[0]*v[trans[1]] for trans in P[s][1])\n",
    "\n",
    "        v[s] = R[s] + gamma*max(q_0, q_1)\n",
    "    \n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31.58508953413495, 38.60400287377479, 44.02416232966445, 54.20158563176306]\n"
     ]
    }
   ],
   "source": [
    "#shorten\n",
    "v = [0] * 4\n",
    "\n",
    "for _ in range(100):\n",
    "    for s in states:\n",
    "        v[s] = R[s] + gamma*max([sum(trans[0]*v[trans[1]] for trans in P[s][a]) for a in actions])\n",
    "    \n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\pi(s) \\leftarrow argmax_a \\sum_{s'\\in S}{P(s' \\ | \\ s,a)v_{\\pi}(s')}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.58508953413495 35.09454620395487\n",
      "42.893337582949 38.60400287377479\n",
      "37.8046259318997 35.09454620395487\n",
      "49.11287398071376 38.60400287377479\n",
      "[1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# optimal policy\n",
    "# once v computed\n",
    "\n",
    "optPolicy = [0]*4\n",
    "\n",
    "for s in states:\n",
    "    q_0 = sum(trans[0]*v[trans[1]] for trans in P[s][0])\n",
    "    q_1 = sum(trans[0]*v[trans[1]] for trans in P[s][1])\n",
    "    \n",
    "    optPolicy[s] = np.argmax([q_0, q_1])\n",
    "    print(q_0, q_1)\n",
    "    \n",
    "print(optPolicy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# shorten\n",
    "optPolicy = [0]*4\n",
    "\n",
    "for s in states:\n",
    "    optPolicy[s] = np.argmax([sum(trans[0]*v[trans[1]] for trans in P[s][a]) for a in actions])\n",
    "    \n",
    "print(optPolicy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = np.random.randint(0, 2, 4) # start, end, numbers\n",
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_value(policy):\n",
    "    v = [0]*4\n",
    "    \n",
    "    for _ in range(100):\n",
    "        for s in states:\n",
    "            v[s] = R[s] + gamma*sum(trans[0]*v[trans[1]] for trans in P[s][policy[s]])\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.232464222377942, 19.839678744431364, 31.462925166669155, 27.85571086998823]\n"
     ]
    }
   ],
   "source": [
    "v = cal_value(policy)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31.58508953413495, 38.60400287377479, 44.02416232966445, 54.20158563176306]\n",
      "[1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    for s in states:\n",
    "        policy[s] = np.argmax([sum(trans[0]*v[trans[1]] for trans in P[s][a]) for a in actions])\n",
    "    \n",
    "    v = cal_value(policy)\n",
    "    \n",
    "print(v)\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions!!!\n",
    "\n",
    "1. 랜덤 policy를 주었을때, Value Function 이 해당 State의 Reward인 이유는? \n",
    "그럴 일은 없다. 해당 문제에서 1번 State가 0 Action을 하게 되면, 계속 자리에 남기 때문에, Value가 갱신 되지 않는 것이다.   \n",
    "\n",
    "** Value Function -> 현 State 에서 Transition 이 계속적으로 일어날때, 쌓이는 Reward 값의 총 합의 기대치 즉 E(G), Expected Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
